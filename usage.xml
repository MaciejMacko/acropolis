<?xml version="1.0" encoding="UTF-8"?>
<!--
 Author: Christophe Gueret <christophe.gueret@bbc.co.uk>
 Author: Mo McRoberts <mo.mcroberts@bbc.co.uk>
 Author: Simeon van der Steen <simeon.vandersteen@bbc.co.uk>
 
 Copyright (c) 2015 BBC
  Licensed under the terms of the Open Government Licence, version 2.0.
  You may obtain a copy of the license at:
	https://www.nationalarchives.gov.uk/doc/open-government-licence/version/2/
-->
<chapter version="5.0" xml:lang="en-gb" xmlns="http://docbook.org/ns" 
	xmlns:xi="http://www.w3.org/2001/XInclude" 
	xmlns:xlink="http://www.w3.org/1999/xlink" xml:id="usage">
	<title>Usage of Acropolis</title>
	<para>This document assumes that all the components of Acropolis are
	correctly compiled and installed on the system</para>
	
	<!-- Anansi -->
	<section xml:id="anansi">
		<title>Anansi</title>
		<para>Anansi is the crawler that will go out on the Web of Data to
		fetch resourses to be index. The list of target ressources is kepts in a
		MySQL relational database</para>
		<para>Anansi needs to have MySQL installed, on Debian-based systems this
		can be done using apt-get</para>
		<programlisting><![CDATA[apt-get install mysql-server]]></programlisting>
		<para>Once MySQL is installed, create a database for anansi</para>
		<programlisting><![CDATA[mysqladmin -u root create anansi]]></programlisting>
		<para>Then run the daemon in the foreground to verify that it is working fine</para>
		<programlisting><![CDATA[/opt/res/sbin/crawld -f]]></programlisting>
		<para>The output should be similar to this:</para>
		<programlisting><![CDATA[
		crawld[7242]: DB: Migrating database to version 1
		crawld[7242]: DB: Migrating database to version 2
		crawld[7242]: DB: Migrating database to version 3
		crawld[7242]: DB: Migrating database to version 4
		crawld[7242]: DB: Migrating database to version 5
		crawld[7242]: [production] crawler 1/1 (thread 1/1), ready
		]]></programlisting>
		<para>Adding a single URI to the queue is done with "crawler-add". For example:</para>
		<programlisting><![CDATA[/opt/res/bin/crawler-add "http://dbpedia.org/resource/Amsterdam"]]></programlisting>
		<programlisting><![CDATA[crawler-add: Notice: added <http://dbpedia.org/resource/Amsterdam> to the crawler queue]]></programlisting>
		<para>When opening a URI Anansi will look at all the other resources 
		being linked to and add them to the queue. By default it will do so for
		all the resources irrespective of their license. To enable the processing
		of only resources published with an Open Data licence it is necessary
		to replace the "rdf" processor by the "lod" processor. This setting is
		set in the configuration file /opt/res/crawl.conf. While tuning that file
		you may also want to have a look at the accepted licenses.</para>
		<programlisting><![CDATA[
		[processor]
		name=lod
		 
		[lod:licenses]
		predicate="http://purl.org/dc/terms/rights"
		predicate="http://purl.org/dc/terms/license"
		predicate="http://purl.org/dc/terms/accessRights"
		predicate="http://creativecommons.org/ns#license"
		predicate="http://www.w3.org/1999/xhtml/vocab#license"
		whitelist="http://creativecommons.org/publicdomain/zero/1.0/"
		]]></programlisting>
		<para>You may also want to tell the crawler to only whitelist certain URI schemes:</para>
		<programlisting><![CDATA[
		[policy:schemes]
		whitelist=http,https,ftp
		]]></programlisting>
    </section>

    <!-- Twine -->
	<section xml:id="twine">
		<title>Twine</title>
		<para>Twine is the component of Acropolis that processes the crawled RDF data. It gets informed
		of the work to be done through an <link xlink:href="https://www.amqp.org/">AMQP</link> queue that
		is populated by the "twine-anansi-bridge" daemon. The outcome of its work is stored in a triple
		store using the SPARQL protocol.</para>
		<para>The first thing to do is to setup a messaging queue and the triple store.</para>
		<programlisting><![CDATA[apt-get install qpidd 4store]]></programlisting>
		<para>We then need to adjust "/opt/res/etc/twine.conf" to tell Twine to 
		use those freshly installed softwares. In the following we assume 4store
		runs a database on the port 9000 (which is the default setup on Debian
		systems)</para>
		<programlisting><![CDATA[
		[mq]
		uri=amqp://127.0.0.1/amq.direct
		
		[sparql]
		update=http://localhost:9000/update/
		data=http://localhost:9000/data/
		query=http://localhost:9000/sparql/
		]]></programlisting>
		<para>Twine will authenticate himself against the message queue as "anonymous@QPID". 
		By default this user will be denied access so we need to adjust /etc/qpid/qpidd.acl to
		change this. Just after the definition of the admin group, and before the catch-all deny
		rule, add the following:</para>
		<programlisting><![CDATA[
		group anon anonymous@QPID
		acl allow anon all
		]]></programlisting>
		<para>Once everything is configured, start the daemon on debug mode to
		ensure it runs fine</para>
		<programlisting><![CDATA[/opt/res/sbin/twine-writerd -f -d]]></programlisting>
		<para>The output should look like:</para>
		<programlisting><![CDATA[
		twine-writerd[10109]: SPARQL query endpoint is <http://localhost:9000/sparql/>
		twine-writerd[10109]: SPARQL update endpoint is <http://localhost:9000/update/>
		twine-writerd[10109]: SPARQL RESTful endpoint is <http://localhost:9000/data/>
		twine-writerd[10109]: establishing connection to <amqp://127.0.0.1/amq.direct>
		twine-writerd[10109]: loading plug-in rdf.so
		twine-writerd[10109]: invoking plug-in initialisation function for /opt/res/lib/twine/rdf.so
		twine-writerd[10109]: rdf plug-in: initialising
		twine-writerd[10109]: registered MIME type: 'application/trig' (RDF TriG)
		twine-writerd[10109]: registered MIME type: 'application/n-quads' (RDF N-Quads)
		twine-writerd[10109]: registered MIME type: 'text/x-nquads' (RDF N-Quads)
		twine-writerd[10109]: loaded plug-in /opt/res/lib/twine/rdf.so
		twine-writerd[10109]: loading plug-in xslt.so
		twine-writerd[10109]: invoking plug-in initialisation function for /opt/res/lib/twine/xslt.so
		twine-writerd[10109]: XSLT: initialising
		twine-writerd[10109]: XSLT: added MIME type 'application/x-example+xml'
		twine-writerd[10109]: registered MIME type: 'application/x-example+xml' (Example XSLT-transformable XML)
		twine-writerd[10109]: loaded plug-in /opt/res/lib/twine/xslt.so
		twine-writerd[10109]: loading plug-in geonames.so
		twine-writerd[10109]: invoking plug-in initialisation function for /opt/res/lib/twine/geonames.so
		twine-writerd[10109]: geonames plug-in: initialising
		twine-writerd[10109]: registered bulk processor for MIME type: 'text/x-geonames-dump' (Geonames dump)
		twine-writerd[10109]: loaded plug-in /opt/res/lib/twine/geonames.so
		twine-writerd[10109]: writerd ready and waiting for messages
		]]></programlisting>
		<para>At that stage Twine is waiting for messages to appear on the AMQP
		message queue. Those messages are to be published by the daemon 
		twine-anansi-brige. This dameon requires Anansi to write its data on a
		data store compatible with S3. The two natural options for this are either
		Amazon AWS or RADOS. On Debian systems, RADOS can be installed with an
		apt-get:</para>
		<programlisting><![CDATA[apt-get install ceph radosgw]]></programlisting>
		<hint>More information about the configarution of Ceph and the Rados
		gateway can be found on <link xlink:href="http://docs.ceph.com/docs/master/radosgw/config/">
		the on-line documentation for Ceph</link></hint>
		<para>The next thing to do is to tell to Anansi and Twine to use S3. To
		do so start by adjusting the configuration of Anansi in /opt/res/etc/craw.conf
		to use S3 instead of the disk-based cache (the default setting) :</para>
		<programlisting><![CDATA[
		[cache]
		;; specify the location of the cache
		; uri=/var/spool/anansi
		uri=s3://[your anansi bucket]/
		username=[your user name]
		password=[your password]
		endpoint=[eg s.ch.internal or s3.amazonaws.com]
		]]></programlisting>
		<para>Then do something similar for Twine in /opt/res/etc/twine.conf.
		The section is not there by default so it needs to be added entirely</para>
		<programlisting><![CDATA[
		;; S3 settings
		[s3]
		access=[your access key]
		secret=[your secret key]
		endpoint=[eg s.ch.internal or s3.amazonaws.com]
		]]></programlisting>
		</section>
		
    <!-- Spindle -->
	<section xml:id="spindle">
		<title>Spindle</title>
		<para>Spindle is a processor module for Twine and Quilt. It generates co-reference 
		proxy objects within a specific set of internal graph. For Twine, it gets activated
		and configured from within the configuration file /opt/res/etc/twine.conf</para>
		<programlisting><![CDATA[
		;; Configuration specifically for the writer daemon (twine-writerd)
		[writerd]
		module=spindle.so
		
		;; Spindle settings
		[spindle]
		graph=[graph-root-uri]
		bucket=[your spindle bucket]
		multigraph=yes
		db=pgsql://mypguser:mypguser@localhost/spindle
		]]></programlisting>
		<para>Spindle requires a database to store the indexes it creates and
		it currently uses PostgreSQL to do that. On Debian systems it can be 
		easilly installed:</para>
		<programlisting><![CDATA[apt-get install postgresql postgresql-client]]></programlisting>
		<para>Follow the instructions on the <link xlink:href="">Debian wiki</link>
		to create a user "mypguser" with the password "mypguser" and a database
		"spindle", or change any of this and adjust the db connection URI
		accordingly.</para>
	</section>
	
	<!-- Quilt -->
	<section xml:id="quilt">
		<title>Quilt</title>
		<para>Quilt is a Web of Data front-end for Acropolis. Quilt needs to be 
		configured with its Spindle module (which is distinct from Twine module 
		for Spindleâ€”both together constitute the Spindle project).</para>
		<para>The Spindle and S3 configuration sections are more or less 
		identical to those for Twine. These settings go to /opt/res/etc/</para>
		<programlisting><![CDATA[
		[quilt]
		engine=spindle
		module=spindle.so
		base=[your graph base]
		 
		[spindle]
		db=pgsql://mypguser:mypguser@localhost/spindle
		bucket=[your spindle bucket]
		
		[s3]
		endpoint=[eg s.ch.internal or s3.amazonaws.com]
		access=[your access key]
		secret=[your secret key]
		verbose=no
		]]></programlisting>
		<para>The next step is to install Apache and configure it to use Quilt
		to reply to requests concerning the ressources. First install Apache2</para>
		<programlisting><![CDATA[apt-get install apache2 libapache2-mod-fastcgi]]></programlisting>
		<para>Then configure an Apache virtual host to accept the http requests, 
		and route them to the FastCGI module (quilt-fastcgid). To do so, create
		a file /etc/apache2/sites-available/quilt with the following content :</para>
		<programlisting><![CDATA[
		<VirtualHost *:80>
				ServerName [hostname]
				DocumentRoot /opt/res/share/quilt/public
		 
				Options FollowSymlinks
		 
				FastCgiExternalServer /opt/res/share/quilt/public/index-virtual.fcgi -socket /tmp/quilt.sock
		 
				CustomLog ${APACHE_LOG_DIR}/quilt.access.log combined
				ErrorLog ${APACHE_LOG_DIR}/quilt.error.log
		 
				RewriteEngine on
				RewriteCond %{LA-U:REQUEST_FILENAME} !-f
				RewriteRule ^(.*)$ /index-virtual.fcgi [NS,L]
		</VirtualHost>
		]]></programlisting>
		<para>Normally you would want Apache to start quilt-fastcgid and then 
		forward the requests to it, but in our case we will start quilt 
		externally (ourselves), so we can use the debugger if needed. We 
		therefore use the FastCgiExternalServer apache directive. The file 
		index-virtual.fcgi should not exist in the document root, so that 
		requests go via /tmp/quilt.sock. If you do want Apache to start 
		quilt, then change the rewriterule to /index.fcgi, this file exists by 
		building quilt, and is a symbolic link to quilt-fastcgid.</para>
		<para>Once this file is created we activate the necessary modules, the
		site 'quilt', and we reload apache:</para>
		<programlisting><![CDATA[
		a2enmod fastcgi
		a2enmod rewrite
		a2ensite quilt
		service apache2 reload
		]]></programlisting>
	</section>
</chapter>
